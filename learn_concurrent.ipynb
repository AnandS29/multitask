{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlp import *\n",
    "from feedback_env import *\n",
    "from learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, data, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        \n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configs for reward and policy learning\n",
    "\n",
    "reward_cfg = {\n",
    "    'n_sample': 100,\n",
    "    'n_epoch': 40,\n",
    "    'lr': 0.001,\n",
    "    'verbose': True,\n",
    "    'eval_freq': 100,\n",
    "    'batch_size': 64,\n",
    "    'split': 0.8,\n",
    "    'log': False,\n",
    "    'action_dim': 1,\n",
    "    'layers': [5,5,5]\n",
    "}\n",
    "\n",
    "policy_cfg = {\n",
    "    'timesteps': 3000,\n",
    "    'verbose': False,\n",
    "    'algo': 'ppo', # not used atm\n",
    "    'log': False,\n",
    "    'action_dim': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x10e944460>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create comparison function\n",
    "f = lambda x: x[0]\n",
    "var_1, var_2 = 0.0, 10.0\n",
    "reward_fn_true = lambda x : f(x)\n",
    "seed = None\n",
    "np.random.seed(seed)\n",
    "noise_fn = lambda x: step_noise(x[0], x_step=0.8, var_1=var_1, var_2=var_2, seed=seed)\n",
    "reward_fn_true_noisy = create_reward_fn_1(f, noise_fn, seed=seed)\n",
    "comparison_fn = create_comparison_fn_1(f, noise_fn, seed=seed)\n",
    "\n",
    "action_dim = policy_cfg['action_dim']\n",
    "register_fb_env(reward_fn_true_noisy, action_dim)\n",
    "\n",
    "if policy_cfg[\"verbose\"]:\n",
    "    verbose = 1\n",
    "else:\n",
    "    verbose = 0\n",
    "\n",
    "timesteps = policy_cfg[\"timesteps\"]\n",
    "algo = policy_cfg[\"algo\"]\n",
    "\n",
    "\n",
    "if verbose: print(\"Learning with PPO\")\n",
    "env = make_vec_env(\"FeedbackEnv-v0\", n_envs=1)\n",
    "if policy_cfg[\"log\"]:\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=verbose, tensorboard_log=\"./log/policy/\"+policy_cfg[\"log\"]+\"/\")\n",
    "else:\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=verbose)\n",
    "model.learn(total_timesteps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
